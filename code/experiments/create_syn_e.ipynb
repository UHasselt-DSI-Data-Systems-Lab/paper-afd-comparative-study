{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa94728-39f9-4c74-9f48-364dc62d8a5c",
   "metadata": {},
   "source": [
    "# SYN$^e$ Generation\n",
    "\n",
    "In this notebook, we generate the SYN$^e$ dataset.\n",
    "\n",
    "## Setup\n",
    "First, load all the files from the RWD dataset. Futhermore, set some configuration parameters if running on an HPC cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033df0c2-fd4c-450d-8c91-9e0789f685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# for Jupyter notebooks: add the path of 'code' to allow importing module\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "data_path = \"../../data\"\n",
    "gt_path = \"../../data/ground_truth.csv\"\n",
    "results_path = \"../../results\"\n",
    "# batch_i is used to parallelize measuring datasets on the HPC cluster\n",
    "batch_i = int(os.getenv(\"PBS_ARRAYID\", 0))\n",
    "# workers is used to parallelize measuring candidate FDs using joblib\n",
    "workers = int(os.getenv(\"PBS_NUM_PPN\", 1))\n",
    "total_batches = 1  # total number of batches that will be run on the HPC\n",
    "# files per batch\n",
    "batch_size = 2\n",
    "# this will be doubled: each dataset will be created as an FD and an non-FD\n",
    "datasets_per_setting = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37dd735",
   "metadata": {},
   "source": [
    "## Define a method to generate SYN$^e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df346-7697-4f93-a34b-1ee2159d0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from synthetic_data import generator as gen\n",
    "from synthetic_data import utils as utils\n",
    "\n",
    "\n",
    "def create_noisy(n: int, max_noise: float = 0.1):\n",
    "    \"\"\"Create n tables with increasing noise. Important settings: both cardinalities 'normal', both skews lower than 1.0, noise ranges from 0.0 to 0.1\"\"\"\n",
    "    settings = {\n",
    "        \"tuples\": random.randint(100, 10000),\n",
    "        \"lhs_cardinality\": 0,\n",
    "        \"rhs_cardinality\": 0,\n",
    "        \"lhs_dist_alpha\": random.randint(58, 100) / 100,\n",
    "        \"lhs_dist_beta\": random.randint(10, 18) / 10,\n",
    "        \"rhs_dist_alpha\": 1.0,\n",
    "        \"rhs_dist_beta\": 1.0,\n",
    "    }\n",
    "    for side in (\"lhs\", \"rhs\"):\n",
    "        while (\n",
    "            utils.beta_skewness(\n",
    "                settings[f\"{side}_dist_alpha\"], settings[f\"{side}_dist_beta\"]\n",
    "            )\n",
    "            > 1.0\n",
    "        ):\n",
    "            settings[f\"{side}_dist_alpha\"] = random.randint(35, 100) / 100\n",
    "            settings[f\"{side}_dist_beta\"] = random.randint(10, 37) / 10\n",
    "\n",
    "    settings[\"lhs_cardinality\"] = random.randint(\n",
    "        int(settings[\"tuples\"] * 0.20), int(settings[\"tuples\"] * 0.75)\n",
    "    )\n",
    "    settings[\"rhs_cardinality\"] = random.randint(\n",
    "        5, int(settings[\"lhs_cardinality\"] / 2)\n",
    "    )\n",
    "\n",
    "    df_set = []\n",
    "    for noise_type in (\"copy\", \"bogus\", \"typo\"):\n",
    "        for noise in range(0, n):\n",
    "            for fd in (True, False):\n",
    "                settings[\"fd\"] = fd\n",
    "                settings[\"noise\"] = random.uniform(\n",
    "                    (noise / n) * max_noise, ((noise + 1) / n) * max_noise\n",
    "                )  # maximum noise is 10%\n",
    "                settings[\"n_type\"] = noise_type\n",
    "                df_set.append((gen.generate_SYN(**settings), copy.deepcopy(settings)))\n",
    "    return df_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015898a-6d7c-460f-a351-95004d94cf87",
   "metadata": {},
   "source": [
    "## Generate SYN$^e$\n",
    "\n",
    "Generate the data using the method defined above. Also, collect and infer the settings used for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228efa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data import inferrence\n",
    "\n",
    "with Parallel(n_jobs=workers) as parallel:\n",
    "    noisy_both = Parallel()(\n",
    "        delayed(create_noisy)(datasets_per_setting) for _ in tqdm(range(batch_size))\n",
    "    )\n",
    "\n",
    "noisy_dfs = []\n",
    "noisy_settings = []\n",
    "noisy_inferred = []\n",
    "for df_set in tqdm(noisy_both):\n",
    "    for df, setting in df_set:\n",
    "        noisy_dfs.append(df)\n",
    "        noisy_settings.append(setting)\n",
    "        noisy_inferred.append(inferrence.infer_settings(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for df_i, noisy_df in enumerate(noisy_dfs):\n",
    "    noisy_df.to_csv(\n",
    "        os.path.join(data_path, \"syn_e\", f\"{batch_i}_{df_i}.csv\"), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a0d8d-6c46-41f4-a6ba-e07e55d95db3",
   "metadata": {},
   "source": [
    "## Calculate SYN AFD measures\n",
    "\n",
    "After generating the tables, calculate the AFD measure scores on the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ce5b0-6777-4cec-97c6-44abc1718ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "\n",
    "from afd_measures import utils as afd_utils\n",
    "\n",
    "measures = [\n",
    "    \"rho\",\n",
    "    \"g2\",\n",
    "    \"g3\",\n",
    "    \"g3_prime\",\n",
    "    \"fraction_of_information\",\n",
    "    \"reliable_fraction_of_information_prime\",\n",
    "    \"smoothed_fraction_of_information\",\n",
    "    \"g1\",\n",
    "    \"g1_prime\",\n",
    "    \"pdep\",\n",
    "    \"tau\",\n",
    "    \"mu_prime\",\n",
    "]\n",
    "\n",
    "to_calulate = [\n",
    "    (df, f\"{batch_i}_{df_i}\", \"lhs\", \"rhs\", measures)\n",
    "    for df_i, df in enumerate(noisy_dfs)\n",
    "]\n",
    "noisy_results = Parallel(n_jobs=workers)(\n",
    "    delayed(afd_utils.parallelize_measuring)(*args) for args in tqdm.tqdm(to_calulate)\n",
    ")\n",
    "# filter out the empty candidate FDs\n",
    "noisy_results_df = pd.DataFrame(noisy_results)\n",
    "# merge it with all the settings\n",
    "noisy_settings = pd.DataFrame(noisy_settings)\n",
    "noisy_inferred = pd.DataFrame(noisy_inferred)\n",
    "noisy_settings[\"table\"] = noisy_settings.apply(\n",
    "    lambda r: f\"{batch_i}_{r.name}\", axis=\"columns\"\n",
    ")\n",
    "noisy_inferred[\"table\"] = noisy_inferred.apply(\n",
    "    lambda r: f\"{batch_i}_{r.name}\", axis=\"columns\"\n",
    ")\n",
    "noisy_results_df = (\n",
    "    noisy_results_df.merge(noisy_settings, on=\"table\", suffixes=(\"\", \"_set\"))\n",
    "    .merge(noisy_inferred, on=\"table\", suffixes=(\"\", \"_inferred\"))\n",
    "    .copy()\n",
    ")\n",
    "# store result to a CSV\n",
    "noisy_results_df.to_csv(\n",
    "    os.path.join(results_path, f\"syn_e_results_{batch_i}.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f67ba2-1fbf-490c-8f35-f4b709dd355d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
