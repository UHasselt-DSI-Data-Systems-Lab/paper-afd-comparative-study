{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cae0f8",
   "metadata": {},
   "source": [
    "# Create RWD$^e$ dataset\n",
    "\n",
    "In this notebook, we aim to pollute perfect, non-trivial functional dependencies to create the RWD$^e$ dataset.\n",
    "\n",
    "## Setup\n",
    "First, load all the files from the RWD dataset. Futhermore, set some configuration parameters if running on an HPC cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b53506e7-a48d-40af-bed6-f37e7494c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30140/1224510875.py:16: DtypeWarning: Columns (3,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rwd_data[file] = pd.read_csv(os.path.join(data_path, \"rwd\", file))\n",
      "/tmp/ipykernel_30140/1224510875.py:16: DtypeWarning: Columns (34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rwd_data[file] = pd.read_csv(os.path.join(data_path, \"rwd\", file))\n",
      "/tmp/ipykernel_30140/1224510875.py:16: DtypeWarning: Columns (6,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rwd_data[file] = pd.read_csv(os.path.join(data_path, \"rwd\", file))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# for Jupyter notebooks: add the path of 'code' to allow importing module\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "from afd_measures import utils as afd_utils\n",
    "\n",
    "data_path = \"../../data\"\n",
    "gt_path = \"../../data/ground_truth.csv\"\n",
    "results_path = \"../../results\"\n",
    "\n",
    "rwd_data = {}\n",
    "for i, file in enumerate(\n",
    "    filter(lambda f: f.endswith(\".csv\"), os.listdir(os.path.join(data_path, \"rwd\")))\n",
    "):\n",
    "    rwd_data[file] = pd.read_csv(os.path.join(data_path, \"rwd\", file))\n",
    "    rwd_data[file].columns = [\n",
    "        afd_utils.clean_colname(c) for c in rwd_data[file].columns\n",
    "    ]\n",
    "\n",
    "if os.path.exists(os.path.join(results_path, \"rwd_results_0.csv\")):\n",
    "    rwd_results = pd.DataFrame()\n",
    "    for file in filter(\n",
    "        lambda f: f.startswith(\"rwd_results_\") and f.endswith(\".csv\"),\n",
    "        os.listdir(results_path),\n",
    "    ):\n",
    "        rwd_results = pd.concat(\n",
    "            [rwd_results, pd.read_csv(os.path.join(results_path, file))]\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Results of RWD are missing. Execute `measure_rwd_afds.ipynb` first.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cb86e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Identify the columns to pollute\n",
    "\n",
    "We are looking for the columns that have to be polluted. That is, all columns that are the RHS of a *perfect* FD but are not the LHS or RHS of *any* AFD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c0edb4-8555-41e8-b5c7-047c6a03f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_gt = rwd_results.query(\"(afd == True) & (trivial_fd == False)\").copy()\n",
    "# add the counts of each column appearing in all FDs contained in the ground truth\n",
    "lhs_counts = (\n",
    "    fds_gt.loc[:, [\"table\", \"lhs\", \"rhs\"]]\n",
    "    .groupby([\"table\", \"lhs\"])\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"rhs\": \"lhs_count\"})\n",
    "    .copy()\n",
    ")\n",
    "rhs_counts = (\n",
    "    fds_gt.loc[:, [\"table\", \"lhs\", \"rhs\"]]\n",
    "    .groupby([\"table\", \"rhs\"])\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"lhs\": \"rhs_count\"})\n",
    "    .copy()\n",
    ")\n",
    "# if a RHS appears as a LHS in the non-perfect ground truth FDs, it cannot be used to introduce noise\n",
    "blocked_rhs_columns = set(fds_gt.query(\"exact_fd == False\").loc[:, \"lhs\"].unique())\n",
    "# create a DataFrame of FDs that could be polluted: perfect FDs, where the RHS is not blocked\n",
    "fds_to_pollute = (\n",
    "    fds_gt.query(\"(exact_fd == True) & (rhs not in @blocked_rhs_columns)\")\n",
    "    .loc[:, [\"table\", \"lhs\", \"rhs\"]]\n",
    "    .copy()\n",
    ")\n",
    "fds_to_pollute = fds_to_pollute.merge(lhs_counts, on=[\"table\", \"lhs\"]).copy()\n",
    "fds_to_pollute = fds_to_pollute.merge(rhs_counts, on=[\"table\", \"rhs\"]).copy()\n",
    "# sort be the counts of appearences, which indicates which RHS / LHS columns to prioritize in order to maximize additional true AFDs after introducing noise\n",
    "fds_to_pollute = fds_to_pollute.sort_values(\n",
    "    [\"rhs_count\", \"lhs_count\"], ascending=[False, True]\n",
    ").copy()\n",
    "columns_to_pollute = set()\n",
    "lhs_to_block = set()\n",
    "# consume the whole list of FDs to pollute, where at each iteration all related FDs are removed (i.e. where the RHS of the consumed FD appears as either one, LHS or RHS)\n",
    "while not fds_to_pollute.empty:\n",
    "    head_table, head_rhs = fds_to_pollute.iloc[0].loc[[\"table\", \"rhs\"]]\n",
    "    # lhs_to_block indicates that we have chosen this column as the LHS of another FD which will be noisiated. Thus, do not use this RHS (due to the ordering, we can be sure that we have introduce at least as much new AFDs already)\n",
    "    if head_rhs not in lhs_to_block:\n",
    "        columns_to_pollute.add((head_table, head_rhs))\n",
    "        lhs_to_block = lhs_to_block | set(\n",
    "            fds_to_pollute.query(\"rhs == @head_rhs\").loc[:, \"lhs\"].unique()\n",
    "        )\n",
    "    fds_to_pollute.drop(\n",
    "        fds_to_pollute.query(\"(lhs == @head_rhs) | (rhs == @head_rhs)\").index,\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4756cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data import generator as sd\n",
    "import pandas as pd\n",
    "\n",
    "fds_to_pollute = []\n",
    "for table, column in columns_to_pollute:\n",
    "    _df: pd.DataFrame = rwd_data[table].copy()\n",
    "    _table_fds = (\n",
    "        fds_gt.query(\"(table == @table) & (rhs == @column)\")\n",
    "        .loc[:, [\"table\", \"lhs\", \"rhs\"]]\n",
    "        .copy()\n",
    "    )\n",
    "    # we are looking for the column with the most noise potential\n",
    "    _settings = {\"tuples\": _df.shape[0]}\n",
    "    _max_noise_index = (\n",
    "        _table_fds.apply(\n",
    "            lambda row: sd.get_noise_potential(\n",
    "                _settings, {0: _df.loc[:, row[\"lhs\"]], 1: _df.loc[:, row[\"rhs\"]]}\n",
    "            ),\n",
    "            axis=\"columns\",\n",
    "        )\n",
    "        .sort_values(ascending=True)\n",
    "        .index[0]\n",
    "    )\n",
    "    fds_to_pollute.append((table, fds_gt.loc[_max_noise_index, \"lhs\"], column))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b260322-e426-4a64-be31-8d92f92a9aa0",
   "metadata": {},
   "source": [
    "## Write the fds to pollute\n",
    "\n",
    "Write the FDs we want to pollute to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56bb587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(os.path.join(results_path, f\"fds_to_pollute.pkl\")):\n",
    "    with open(os.path.join(results_path, f\"fds_to_pollute.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(fds_to_pollute, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a318d46b-77b0-4867-94c2-bdabf686addb",
   "metadata": {},
   "source": [
    "## Pollute RWD\n",
    "\n",
    "Introduce errors into the RWD tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06992ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "from synthetic_data import generator as sd\n",
    "\n",
    "noise_funcs = {\n",
    "    \"copy\": sd.introduce_noise_copy,\n",
    "    \"bogus\": sd.introduce_noise_bogus,\n",
    "    \"typo\": sd.introduce_noise_typo,\n",
    "    \"copy-lhs\": sd.introduce_lhs_noise_copy,\n",
    "}\n",
    "\n",
    "noisy_dfs = {\n",
    "    func: {nl: {} for nl in (0.01, 0.02, 0.05, 0.1)} for func in noise_funcs.keys()\n",
    "}\n",
    "for table_name, lhs_name, rhs_name in fds_to_pollute:\n",
    "    original_df = rwd_data[table_name].copy()\n",
    "    potential = sd.get_noise_potential_df(original_df, lhs_name, rhs_name)\n",
    "    trimmed_df = original_df.loc[:, [lhs_name, rhs_name]].dropna().copy()\n",
    "    clean = {0: trimmed_df.iloc[:, 0].to_list(), 1: trimmed_df.iloc[:, 1].to_list()}\n",
    "    for noise_level in (0.01, 0.02, 0.05, 0.1):\n",
    "        if potential <= noise_level:\n",
    "            print(\n",
    "                f\"Cannot introduce noise of {noise_level} to {table_name}, {lhs_name} -> {rhs_name}. Potential of {potential} is too low.\"\n",
    "            )\n",
    "        else:\n",
    "            settings = {\n",
    "                \"noise\": noise_level,\n",
    "                \"tuples\": trimmed_df.shape[0],\n",
    "                \"rhs_cardinality\": trimmed_df.iloc[:, 1].nunique(),\n",
    "            }\n",
    "            for noise_type, noise_func in noise_funcs.items():\n",
    "                noisy = noise_func(settings, copy.deepcopy(clean))\n",
    "                if table_name in noisy_dfs[noise_type][noise_level]:\n",
    "                    noisy_df = noisy_dfs[noise_type][noise_level][table_name]\n",
    "                else:\n",
    "                    noisy_df = original_df.copy()\n",
    "                trimmed_df.loc[:, rhs_name] = noisy[1]\n",
    "                noisy_df.update(trimmed_df, overwrite=True)\n",
    "                noisy_dfs[noise_type][noise_level][table_name] = noisy_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60fae2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "for noise_type, noise_level in itertools.product(\n",
    "    noise_funcs.keys(), (0.01, 0.02, 0.05, 0.1)\n",
    "):\n",
    "    for table_name, df in noisy_dfs[noise_type][noise_level].items():\n",
    "        df.to_csv(\n",
    "            os.path.join(\n",
    "                data_path, \"rwd_e\", f\"polluted_{noise_type}_{noise_level}_{table_name}\"\n",
    "            ),\n",
    "            index=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
