{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa94728-39f9-4c74-9f48-364dc62d8a5c",
   "metadata": {},
   "source": [
    "# SYN$^s$ Generation\n",
    "\n",
    "In this notebook, we generate the SYN$^s$ dataset.\n",
    "\n",
    "## Setup\n",
    "First, load all the files from the RWD dataset. Futhermore, set some configuration parameters if running on an HPC cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033df0c2-fd4c-450d-8c91-9e0789f685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# for Jupyter notebooks: add the path of 'code' to allow importing module\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "data_path = \"../../data\"\n",
    "gt_path = \"../../data/ground_truth.csv\"\n",
    "results_path = \"../../results\"\n",
    "# batch_i is used to parallelize measuring datasets on the HPC cluster\n",
    "batch_i = int(os.getenv(\"PBS_ARRAYID\", 0))\n",
    "# workers is used to parallelize measuring candidate FDs using joblib\n",
    "workers = int(os.getenv(\"PBS_NUM_PPN\", 1))\n",
    "total_batches = 1  # total number of batches that will be run on the HPC\n",
    "# files per batch\n",
    "batch_size = 1\n",
    "# this will be doubled: each dataset will be created as an FD and an non-FD\n",
    "datasets_per_setting = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37dd735",
   "metadata": {},
   "source": [
    "## Define a method to generate SYN$^s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df346-7697-4f93-a34b-1ee2159d0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from synthetic_data import generator as gen\n",
    "from synthetic_data import utils as utils\n",
    "\n",
    "\n",
    "def create_predominant_RHS(n: int, skew_lookup: pd.DataFrame, max_skew: float = 10.0):\n",
    "    \"\"\"Create a predominant RHS. Important settings: LHS skew lower than 1.0, RHS cardinality low, RHS skew higher than 1.0.\"\"\"\n",
    "    settings = {\n",
    "        \"tuples\": random.randint(100, 10000),\n",
    "        \"lhs_cardinality\": 0,\n",
    "        \"rhs_cardinality\": 0,\n",
    "        \"lhs_dist_alpha\": random.randint(58, 100) / 100,\n",
    "        \"lhs_dist_beta\": random.randint(10, 18) / 10,\n",
    "        \"rhs_dist_alpha\": 1.0,\n",
    "        \"rhs_dist_beta\": 1.0,\n",
    "        \"noise\": random.uniform(0.005, 0.02),\n",
    "    }\n",
    "    settings[\"lhs_cardinality\"] = random.randint(\n",
    "        int(settings[\"tuples\"] * 0.20), int(settings[\"tuples\"] * 0.75)\n",
    "    )\n",
    "    while (\n",
    "        utils.beta_skewness(settings[\"lhs_dist_alpha\"], settings[\"lhs_dist_beta\"]) > 0.5\n",
    "    ):\n",
    "        settings[\"lhs_dist_alpha\"] = random.randint(58, 100) / 100\n",
    "        settings[\"lhs_dist_beta\"] = random.randint(10, 18) / 10\n",
    "    df_set = []\n",
    "    for rhs_pred in range(0, n):\n",
    "        lower_skew = (max_skew * rhs_pred) / n\n",
    "        upper_skew = (max_skew * rhs_pred + max_skew) / n\n",
    "        alpha, beta = random.choice(\n",
    "            skew_lookup.query(f\"skew > {lower_skew} and skew < {upper_skew}\").index\n",
    "        )\n",
    "        settings[\"rhs_dist_alpha\"] = alpha\n",
    "        settings[\"rhs_dist_beta\"] = beta\n",
    "        settings[\"rhs_cardinality\"] = random.randint(\n",
    "            5, int(settings[\"lhs_cardinality\"] / 2)\n",
    "        )\n",
    "        for fd in (True, False):\n",
    "            settings[\"fd\"] = fd\n",
    "            df_set.append((gen.generate_SYN(**settings), copy.deepcopy(settings)))\n",
    "    return df_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015898a-6d7c-460f-a351-95004d94cf87",
   "metadata": {},
   "source": [
    "## Generate SYN$^s$\n",
    "\n",
    "Generate the data using the method defined above. Also, collect and infer the settings used for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228efa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data import inferrence\n",
    "\n",
    "with Parallel(n_jobs=workers) as parallel:\n",
    "    predominant_both = Parallel()(\n",
    "        delayed(create_predominant_RHS)(datasets_per_setting)\n",
    "        for _ in tqdm(range(batch_size))\n",
    "    )\n",
    "\n",
    "predominant_dfs = []\n",
    "predominant_settings = []\n",
    "predominant_inferred = []\n",
    "for df_set in tqdm(predominant_both):\n",
    "    for df, setting in df_set:\n",
    "        predominant_dfs.append(df)\n",
    "        predominant_settings.append(setting)\n",
    "        predominant_inferred.append(inferrence.infer_settings(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for df_i, predominant_df in enumerate(predominant_dfs):\n",
    "    predominant_df.to_csv(\n",
    "        os.path.join(data_path, \"syn_s\", f\"{batch_i}_{df_i}.csv\"), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a0d8d-6c46-41f4-a6ba-e07e55d95db3",
   "metadata": {},
   "source": [
    "## Calculate SYN AFD measures\n",
    "\n",
    "After generating the tables, calculate the AFD measure scores on the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ce5b0-6777-4cec-97c6-44abc1718ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "\n",
    "from afd_measures import utils as afd_utils\n",
    "\n",
    "to_calulate = [\n",
    "    (df, f\"{batch_i}_{df_i}\", \"lhs\", \"rhs\", afd_utils.measure_order)\n",
    "    for df_i, df in enumerate(predominant_dfs)\n",
    "]\n",
    "predominant_results = Parallel(n_jobs=workers)(\n",
    "    delayed(afd_utils.parallelize_measuring)(*args) for args in tqdm.tqdm(to_calulate)\n",
    ")\n",
    "# filter out the empty candidate FDs\n",
    "predominant_results_df = pd.DataFrame(predominant_results)\n",
    "# merge it with all the settings\n",
    "predominant_settings = pd.DataFrame(predominant_settings)\n",
    "predominant_inferred = pd.DataFrame(predominant_inferred)\n",
    "predominant_settings[\"table\"] = predominant_settings.apply(\n",
    "    lambda r: f\"{batch_i}_{r.name}\", axis=\"columns\"\n",
    ")\n",
    "predominant_inferred[\"table\"] = predominant_inferred.apply(\n",
    "    lambda r: f\"{batch_i}_{r.name}\", axis=\"columns\"\n",
    ")\n",
    "predominant_results_df = (\n",
    "    predominant_results_df.merge(\n",
    "        predominant_settings, on=\"table\", suffixes=(\"\", \"_set\")\n",
    "    )\n",
    "    .merge(predominant_inferred, on=\"table\", suffixes=(\"\", \"_inferred\"))\n",
    "    .copy()\n",
    ")\n",
    "# store result to a CSV\n",
    "predominant_results_df.to_csv(\n",
    "    os.path.join(results_path, f\"syn_s_results_{batch_i}.csv\"), index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
